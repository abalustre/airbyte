#
# Copyright (c) 2021 Airbyte, Inc., all rights reserved.
#

import traceback
from datetime import datetime
from typing import Generator, Iterable, Mapping

from .client import Client
from base_python import AirbyteLogger, Source

from airbyte_protocol import (
    AirbyteCatalog,
    AirbyteConnectionStatus,
    AirbyteMessage,
    AirbyteRecordMessage,
    ConfiguredAirbyteCatalog,
    Status,
    Type,
)



class SourceFileTC(Source):
    """This source aims to provide support for readers of different file formats stored in various locations.

    It is optionally using s3fs, gcfs or smart_open libraries to handle efficient streaming of very large files
    (either compressed or not).

    Supported examples of URL this can accept are as follows:
    ```
        s3://my_bucket/my_key
        s3://my_key:my_secret@my_bucket/my_key
        gs://my_bucket/my_blob
        azure://my_bucket/my_blob (not tested)
        hdfs:///path/file (not tested)
        hdfs://path/file (not tested)
        webhdfs://host:port/path/file (not tested)
        ./local/path/file
        ~/local/path/file
        local/path/file
        ./local/path/file.gz
        file:///home/user/file
        file:///home/user/file.bz2
        [ssh|scp|sftp]://username@host//path/file
        [ssh|scp|sftp]://username@host/path/file
        [ssh|scp|sftp]://username:password@host/path/file
    ```

    The source reader currently leverages `read_csv` but will be extended to readers of different formats for
    more potential sources as described below:
    https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html
    - read_json
    - read_html
    - read_excel
    - read_feather
    - read_parquet
    - read_orc
    - read_pickle

    All the options of the readers are exposed to the configuration file of this connector so it is possible to
    override header names, types, encoding, etc

    Note that this implementation is handling `url` target as a single file at the moment.
    We will expand the capabilities of this source to discover and load either glob of multiple files,
    content of directories, etc in a latter iteration.
    """

    client_class = Client

    def _get_client(self, config: Mapping):
        """Construct client"""
        client = self.client_class(**config)

        return client

    def check(self, logger, config: Mapping) -> AirbyteConnectionStatus:
        """
        Check involves verifying that the specified file is reachable with
        our credentials.
        """
        client = self._get_client(config)
        logger.info(f"Checking access to {client.full_url}...")
        try:
            if client.is_folder:
                files = client.list_files_in_folder()

                for file_name in files:
                    url_file = client.full_url.replace("*", file_name)
                    file = client.open(url_file,binary=client.binary_source)
                    file.close()

            else:
                file = client.open(binary=client.binary_source)
                file.close()    
            
            return AirbyteConnectionStatus(status=Status.SUCCEEDED)
        
        except Exception as err:
            reason = f"Failed to load {client.full_url}: {repr(err)}\n{traceback.format_exc()}"
            logger.error(reason)
            return AirbyteConnectionStatus(status=Status.FAILED, message=reason)

    def discover(self, logger: AirbyteLogger, config: Mapping) -> AirbyteCatalog:
        """
        Returns an AirbyteCatalog representing the available streams and fields in this integration. For example, given valid credentials to a
        Remote CSV File, returns an Airbyte catalog where each csv file is a stream, and each column is a field.
        """    
        client = self._get_client(config)
        name = client.stream_name
        logger.info(f"Discovering schema of {name} at {client.full_url}...")
        file = None
        if client.is_folder:
            file_name = client.list_files_in_folder()[0]
            file = client.full_url.replace("*", file_name)
        try:
            streams = list(client.streams(file=file))
        except Exception as err:
            reason = f"Failed to discover schemas of {name} at {client.full_url}: {repr(err)}\n{traceback.format_exc()}"
            logger.error(reason)
            raise err
        
        return AirbyteCatalog(streams=streams,name=name)

    def read(
        self, logger: AirbyteLogger, config: Mapping, catalog: ConfiguredAirbyteCatalog, state_path: Mapping[str, any]
    ) -> Generator[AirbyteMessage, None, None]:
        """Returns a generator of the AirbyteMessages generated by reading the source with the given configuration, catalog, and state."""
        client = self._get_client(config)
        fields = self.selected_fields(catalog)        
        name = client.stream_name
        
        logger.info(f"Reading {name} ({client.full_url})...")
    
        if client.is_folder:
            files = client.list_files_in_folder()
            file_names = [client.full_url.replace("*", file_name) for file_name in files]
        else:
            file_names = [client.full_url]
        
        for file_name in file_names:
            try:
                for row in client.read(fields=fields,file=file_name):
                    record = AirbyteRecordMessage(stream=name, data=row, emitted_at=int(datetime.now().timestamp()) * 1000)
                    yield AirbyteMessage(type=Type.RECORD, record=record)
            except Exception as err:
                reason = f"Failed to read data of {name} at {client.full_url}: {repr(err)}\n{traceback.format_exc()}"
                logger.error(reason)
                raise err

    @staticmethod
    def selected_fields(catalog: ConfiguredAirbyteCatalog) -> Iterable:
        for configured_stream in catalog.streams:
            yield from configured_stream.stream.json_schema["properties"].keys()
